{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train a more complex convolutional model to predict log fold change f K27act from sequence and atac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/kal/TF_models/bin/')\n",
    "sys.path.append('/home/kal/K27act_models/convolution_model/')\n",
    "sys.path.append('/home/kal/K27act_models/cg_model/')\n",
    "sys.path.append('/home/thouis/basenji_embeddings')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import tf_memory_limit\n",
    "from zinb import ZINB\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import sequence\n",
    "import ucscgenome\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datagen\n",
    "import sequence\n",
    "import helper\n",
    "import viz_sequence\n",
    "import pickle\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, SpatialDropout1D, Conv1D, Lambda, Dropout\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load in the DNA\n",
    "genome = ucscgenome.Genome('/home/kal/.ucscgenome/hg19.2bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load in ATAC data\n",
    "atac_path = '/home/kal/K27act_models/GM_data/ATAC/atac_average.hdf5'\n",
    "atac = h5py.File(atac_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load in (semi) preprocessed data\n",
    "peaks_path = '/home/kal/K27act_models/GM_data/k27act_training_regions.bed'\n",
    "peaks = pd.read_table(peaks_path, header=None)\n",
    "peaks.columns='chr start end name score atac k27act nucs'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# facts about the data\n",
    "num_training_samples = len(peaks[(peaks.chr != 'chr8')])\n",
    "print('{} training samples'.format(num_training_samples))\n",
    "\n",
    "num_testing_samples = len(peaks[(peaks.chr == 'chr8') & (peaks.index%2 == 0)])\n",
    "print('{} testing samples'.format(num_testing_samples))\n",
    "\n",
    "num_validaiton_samples = len(peaks[(peaks.chr == 'chr8') & (peaks.index%2 == 1)])\n",
    "print('{} validation samples'.format(num_validaiton_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# directory stuff\n",
    "out_dir = '/home/kal/K27act_models/convolution_model/'\n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = os.path.join(out_dir, timestr + '_conv_fold')\n",
    "os.makedirs(out_path)\n",
    "# make a file system\n",
    "weights_path = os.path.join(out_path, 'intermediate_weights')\n",
    "os.makedirs(weights_path)\n",
    "history_path = os.path.join(out_path, 'history')\n",
    "os.makedirs(history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "batch_size=32\n",
    "seqs = Input(batch_shape=(batch_size, 1024, 5))\n",
    "\n",
    "def add_RC(x):\n",
    "    RC = K.concatenate([x[:, ::-1, :1], x[:, ::-1, 1:][::-1]], axis=2)\n",
    "    return K.concatenate([x, RC], axis=0)\n",
    "def add_RC_shape(s):\n",
    "    return 2*s[0], s[1], s[2]\n",
    "\n",
    "conv = Lambda(add_RC, name='add_rc', output_shape=add_RC_shape)(seqs)\n",
    "\n",
    "for k, n in [[32, 3], [32, 32], [32, 3], [32, 3], [32,3]]:\n",
    "    conv = Conv1D(k, n)(conv)\n",
    "    conv = SpatialDropout1D(.05)(conv)\n",
    "\n",
    "out = Dense(32)(conv) \n",
    "out = Dropout(.1)(out)\n",
    "out = Dense(16)(out)\n",
    "out = Dropout(.1)(out)\n",
    "out = Dense(1)(out)\n",
    "\n",
    "def max_by_direction(x):\n",
    "    forward_max = K.max(x[:x.shape[0]//2, :, :], axis=1)\n",
    "    reverse_max = K.max(x[x.shape[0]//2:, ::-1, :], axis=1)\n",
    "    return K.maximum(forward_max, reverse_max)\n",
    "def max_by_direction_shape(s):\n",
    "    return s[0]//2, 1\n",
    "\n",
    "predictions = Lambda(max_by_direction, name='max_by_direciton', output_shape=max_by_direction_shape)(out)\n",
    "model = Model(inputs=seqs, outputs=predictions)\n",
    "\n",
    "#plot model\n",
    "plot_model(model, to_file=os.path.join(out_path, 'model.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "opt_1 = Adam(beta_1=0.95, lr=0.0005, epsilon=.1)\n",
    "\n",
    "    \n",
    "    \n",
    "model.compile(optimizer=opt_1, loss='mean_absolute_error') \n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=100)\n",
    "filepath = os.path.join(weights_path, 'weights-1-{epoch:02d}-{val_loss:.3f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "losses_1 = model.fit_generator(datagen.batch_gen(peaks, mode='train'), steps_per_epoch=num_training_samples//batch_size, \n",
    "                             epochs=20, callbacks=[early_stop, checkpoint], validation_data=datagen.batch_gen(peaks,mode='val'), \n",
    "                             validation_steps=num_validaiton_samples//batch_size, verbose=2)\n",
    "\n",
    "# drop learning rate\n",
    "opt_2 = Adam(beta_1=0.95, lr=0.00005, epsilon=.001)\n",
    "model.compile(optimizer=opt_2, loss='mean_absolute_error') \n",
    "\n",
    "filepath = os.path.join(weights_path, 'weights-2-{epoch:02d}-{val_loss:.3f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "losses_2 = model.fit_generator(datagen.batch_gen(peaks, mode='train'), steps_per_epoch=num_training_samples//batch_size, \n",
    "                             epochs=20, callbacks=[early_stop, checkpoint], validation_data=datagen.batch_gen(peaks,mode='val'), \n",
    "                             validation_steps=num_validaiton_samples//batch_size, verbose=2)\n",
    "\n",
    "\n",
    "# drop it again\n",
    "opt_3 = Adam(beta_1=0.95, lr=0.000005, epsilon=.00001)\n",
    "model.compile(optimizer=opt_3, loss='mean_absolute_error') \n",
    "\n",
    "filepath = os.path.join(weights_path, 'weights-3-{epoch:02d}-{val_loss:.3f}.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "losses_3 = model.fit_generator(datagen.batch_gen(peaks, mode='train'), steps_per_epoch=num_training_samples//batch_size, \n",
    "                             epochs=20, callbacks=[early_stop, checkpoint], validation_data=datagen.batch_gen(peaks,mode='val'), \n",
    "                             validation_steps=num_validaiton_samples//batch_size, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# final save\n",
    "model.save(os.path.join(out_path, 'final_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_list = [losses_1, losses_2, losses_3]\n",
    "losses=dict()\n",
    "for key in losses_1.history:\n",
    "    data_group=list()\n",
    "    for loss_obj in loss_list:\n",
    "        data_group.append(loss_obj.history[key])\n",
    "    losses[key] = np.asarray(data_group).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write out history\n",
    "pickle.dump(losses, open(os.path.join(history_path, 'history.pk'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# look at loss\n",
    "plt.plot(losses['val_loss'], label='validation')\n",
    "plt.plot(losses['loss'], label='training')\n",
    "plt.title('Loss for training model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim((0, 30))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
